ğŸš€ Features

âœ… Scrapes last 5 oldest blog articles from BeyondChats

âœ… Cleans and normalizes scraped text

âœ… Stores data in SQLite

âœ… RESTful CRUD APIs

âœ… One-click refresh scrape endpoint

âœ… Batch replace API

âœ… CORS enabled (frontend ready)

ğŸ›  Tech Stack

Node.js

Express.js

SQLite3

Axios (HTTP requests)

Cheerio (HTML parsing)

CORS

ğŸ“‚ Project Structure
.
â”œâ”€â”€ db.js            # SQLite DB initialization
â”œâ”€â”€ scraper.js       # Blog scraping logic
â”œâ”€â”€ server.js        # Express server + APIs
â”œâ”€â”€ articles.db      # SQLite database (auto-created)
â”œâ”€â”€ package.json
â””â”€â”€ README.md

ğŸ§± Database Schema

Table: articles

Column	Type	Description
id	INTEGER	Primary key
title	TEXT	Article title
content	TEXT	Article content
source_url	TEXT	Original blog URL
is_updated	INTEGER	0 = false, 1 = true
created_at	DATETIME	Auto timestamp
ğŸ“¦ Installation
git clone https://github.com/your-username/beyondchats-scraper.git
cd beyondchats-scraper
npm install

â–¶ï¸ Run the Server
node server.js


Server will start at:

http://localhost:4000

ğŸ”„ Scrape & Initialize Data
Scrape oldest 5 blogs and refresh DB
GET /scrape-init


What it does:

Deletes existing articles

Scrapes oldest 5 blog posts

Inserts fresh data

Response

{
  "message": "Old articles deleted. Fresh articles inserted.",
  "count": 5
}

ğŸ“¡ API Endpoints
â¤ Get All Articles
GET /api/articles

â¤ Get Article by ID
GET /api/articles/:id

â¤ Create Article
POST /api/articles


Body

{
  "title": "Sample Title",
  "content": "Sample Content",
  "source_url": "https://example.com"
}

â¤ Update Article
PUT /api/articles/:id


Body

{
  "title": "Updated Title",
  "content": "Updated Content",
  "is_updated": 1
}

â¤ Delete Article
DELETE /api/articles/:id

â¤ Replace All Articles (Batch)
POST /api/articles/replace-all


Body

{
  "articles": [
    {
      "title": "Title 1",
      "content": "Content 1",
      "source_url": "https://example.com",
      "is_updated": true
    }
  ]
}
